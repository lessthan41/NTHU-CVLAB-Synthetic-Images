# Diffusion-based Synthetic Anomalies for Industrial Inspection (DSAII)
```clike=
  synthetic_tidy_v2
  ├─ audio cable
  │  ├─ img_url.json
  │  │
  │  ├─ normal
  │  │  ├─ 000.png
  │  │  ├─ 001.png
  │  │    ...
  │  │  └─ 019.png
  │  │
  │  ├─ anomaly
  │  │  ├─ 000.png
  │  │  ├─ 001.png
  │  │    ...
  │  │  └─ 017.png
  │  │
  │  └─ mask
  │     ├─ 000.png
  │     ├─ 001.png
  │       ...
  │     └─ 017.png
  ├─ audiopipe
  ...
```

## Why use DSAII?
DSAII dataset contains normal images with **large intra-class variation**, while defective images are generated by diffusion model. This setting aims to evaluate the model’s **generalizability** and **few-shot capabilities**. The model needs to learn the abstract pattern of the object/texture and the its normal appearance to achieve better performance.

## Dataset Structure
DSAII dataset are obtained by crawling and arranging similar images from the web. Each class (0\~77) contains 10\~20 images without defect, which is saved under path `synthetic_tidy_v2/{CLASS}/normal`. The image reference link is listed in `img_url.json`. The whole dataset contains 2906 images (including normal and defect images).

For each normal images, we apply Stable Diffusion for defect inpainting respectively. The inpainting mask is acquired by random sampling ground_truth mask from [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) dataset, then applying augmentations as our input inpainting masks. The inpainting masks are saved under path `synthetic_tidy_v2/{CLASS}/mask`. The output defect images are saved under path `synthetic_tidy_v2/{CLASS}/defect`. The failure cases are eliminated.

<img width="921" alt="image" display="block" margin="auto" src="https://github.com/lessthan41/NTHU-CVLAB-Synthetic-Images/assets/43234148/2f58dd38-e821-46ec-9324-4bf6ceab5d87">

<!--- ## Dataset Download
For downloading our synthetic dataset, please visit <a href="https://drive.google.com/file/d/148yCBS_6I7WqSMbgY4LTKq97Nb5NTS2L/view?usp=sharing" target="_blank">synthetic_tidy_v2</a>. For the convenience, <a href="https://drive.google.com/file/d/1j4iDajm9rt1Pj0Numpn0-4tT6rnyM7EL/view?usp=sharing" target="_blank">synthetic_mvtec_like</a> is also provided with same structure as MVTecAD dataset. -->

## Before Started
Due to Copyright Law, we should be careful when crawling images from the internet. Please make sure the images are available for free or contact author for further authorization.

## Instructions
### Phase 1: Crawler
* Step 1: Prepare representative images for each class you want to create.    
For crawling bunches of similar images, first prepare one image for each class, for example:
```clike=
  /path/to/dir
  ├─ audiopipe
  │  └─ 000.png
  ├─ bucket
  │  └─ 000.png
  ...
```
* Step 2: Prepare a Google Chrome browser.    
* Step 3: Download `chromedriver.exe` from <a href="https://chromedriver.chromium.org/downloads" target="_blank">official website</a>.
* Step 4: Edit `SRC`, `DST` path to directory in `crawler_picture.py` then execute `python crawler_picture.py`.
After executing `crawler_picture.py`, you will get output directory, for example:
```clike=
  /path/to/dir
  ├─ audiopipe
  │  ├─ 000.png
  │  ├─ 001.png
  │  │    ...
  │  └─ 019.png
  ├─ bucket
  │  ├─ 000.png
  ...
```

### Phase 2: Ground Truth Mask Generation
* Step 1: Follow <a href="https://github.com/facebookresearch/segment-anything" target="_blank">Segment Anything</a> to acquiring object mask, for furthur generating ground-truth mask.
The output mask candidates should looks like:
```clike=
  /object/mask/candidates/
  ├─ audiopipe
  │  ├─ 000
  │  │  ├─ 0.png
  │  │  ├─ 1.png
  │  │    ...
  │  │  └─ 16.png
  │  ├─ 001
  │  │  ├─ 0.png
  │  │    ...
  │  │  └─ 8.png
  │  │    ...
  │  └─ 019
  │     ├─ 0.png
  │      ...
  ├─ bucket
  │  ├─ 0.png
  ...
```
* Step 2: Edit `SRC`, `DST` path to directory in `mask_extractor.py` then execute `python mask_extractor.py`. It will output the final object mask.
* Step 3: Prepare a bunch of mask you want to use, in this case we use [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad) testing set mask.
* Step 4: Update `SKIP` list in `gen_mask.py` for skipping object mask when augmenting ground truth mask. `SRC`, `MASK`, `DST` for input/mask/output directories.
* Step 5: Run `python gen_mask.py`

### Phase 3: Diffusion Inpainting
* Step 1: Generate masks of anomaly region from exist masks or prepare your own masks
* Step 2: Download and install stable diffusion web-ui follow the instructions in [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui).
* Step 3: Apply inpaint function to the image with corresponding mask given the prompt (in “img2img” block and choose the “batch” section).
```
  prompt for objects: broken, cracked, severely shattered
  prompt for textures: broken, cracked, severely shattered(“stains, holes” are applied on classes that don’t go well with default prompts)
```

## Experiment
We test synthetic dataset on [Patchcore](https://github.com/amazon-science/patchcore-inspection) (CVPR, 2022), [SimpleNet](https://github.com/DonaldRR/SimpleNet) (CVPR, 2023). The performance is record as follow.

| Method | Mean Image AUC (%) | Mean Pixel AUC (%)|
|-----|-----|--------|
|Patchcore | 57.2 |   87.5    |
|SimpleNet  |69.1    |   65.3  |

## NTHU CVLAB
For more details, please visit [CVLAB](https://cv.cs.nthu.edu.tw/).
<br><br>
CHENG-YU HO: `lesthan41@gapp.nthu.edu.tw` <br>
YU-HSUAN HSIEH: `ss111062646@gapp.nthu.edu.tw`

## License
MIT Licence
